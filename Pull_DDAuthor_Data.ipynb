{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Author Data \n",
    "For each DD post, pull data for authors that is included in the bot-generated post. \n",
    "- Earliest WSB appearance \n",
    "- Total comments up until post date time \n",
    "- Total posts up intil post date time\n",
    "- Links to previous DD posts - pull post ID for previous DD posts \n",
    "\n",
    "For each DD post author, pull all previous posts (including flair to identify DD posts) and all previous comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import praw \n",
    "import pandas as pd \n",
    "import datetime as dt \n",
    "from datetime import timedelta\n",
    "import numpy as np \n",
    "from psaw import PushshiftAPI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up \n",
    "1. Initialize instance of Pushshift API wrapper\n",
    "2. Load dd post data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reddit and pushshift API for accessing posts\n",
    "reddit = praw.Reddit(client_id = 'rAfwBiShkge-tw',\n",
    "                    client_secret = '1rcUS_cY1tZ_uDGwJdpEIe-vSow',\n",
    "                    user_agent = 'wsb_app',\n",
    "                    username = 'Real_Measurement_493', \n",
    "                    password = 'stella96')\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_df = pd.read_pickle('/Volumes/Elements/Research/Reddit_Credibility/Data/dd_df_sample.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull all submissions made by dd post authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of unique authors \n",
    "authors = dd_df.author.unique()\n",
    "authors = list(filter(None, authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize post list and comment list \n",
    "author_post_list = []\n",
    "author_comment_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "On submission 0 out of 6258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimmiegeorge/opt/anaconda3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/Users/kimmiegeorge/opt/anaconda3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "On submission 500 out of 6258\n",
      "============================\n",
      "On submission 1000 out of 6258\n",
      "============================\n",
      "On submission 1500 out of 6258\n",
      "============================\n",
      "On submission 2000 out of 6258\n",
      "============================\n",
      "On submission 2500 out of 6258\n",
      "============================\n",
      "On submission 3000 out of 6258\n",
      "============================\n",
      "On submission 3500 out of 6258\n",
      "============================\n",
      "On submission 4000 out of 6258\n",
      "============================\n",
      "On submission 4500 out of 6258\n",
      "============================\n",
      "On submission 5000 out of 6258\n",
      "============================\n",
      "On submission 5500 out of 6258\n",
      "============================\n",
      "On submission 6000 out of 6258\n"
     ]
    }
   ],
   "source": [
    "# loop through authors \n",
    "for i in range(0, len(authors)):\n",
    "    \n",
    "    a = authors[i]\n",
    "    \n",
    "    # print counter \n",
    "    if i % 500 == 0:\n",
    "        print('============================')\n",
    "        print('On submission ' + str(i) + ' out of ' + str(len(authors)))\n",
    "    \n",
    "    # search for all submissions \n",
    "    sub_search = api.search_submissions(subreddit = 'wallstreetbets', author = a, \n",
    "                                       filter = ['url', 'author', 'title', 'body', 'created', \n",
    "                                                'num_comments', 'score', 'link_flair_text'])\n",
    "    # loop through sub_search \n",
    "    for submission in sub_search:\n",
    "        \n",
    "        # if submission hasn't been removed\n",
    "        if submission.is_robot_indexable:\n",
    "        \n",
    "            # pull all needed data\n",
    "            title = submission.title\n",
    "            author = submission.author\n",
    "            score = submission.score\n",
    "            submission_id = submission.id\n",
    "            url = submission.url\n",
    "            comms_num = submission.num_comments\n",
    "            created = submission.created\n",
    "            flair = submission.link_flair_text \n",
    "            body = submission.selftext\n",
    "\n",
    "            # append to author post list \n",
    "            author_post_list.append((a, title, author, score, submission_id, url, \n",
    "                                    comms_num, created, flair, body))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to df \n",
    "author_post_df = pd.DataFrame(author_post_list, columns = ['author', 'title', 'author2', 'score', \n",
    "                                                          'submission_id', 'url', 'comms_num', 'created', 'flair', 'body'])\n",
    "\n",
    "\n",
    "# remove duplicate posts (from re-starting loop)\n",
    "author_post_df = author_post_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save author post df \n",
    "author_post_df.to_pickle('/Volumes/Elements/Research/Reddit_Credibility/Data/posts_by_dd_authors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run search for Kremfloete - did not work \n",
    "dd_df['author_str'] = dd_df['author'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = dd_df[dd_df['author_str'] == 'Kremfloete'].author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for all submissions \n",
    "sub_search = api.search_submissions(subreddit = 'wallstreetbets', author = auth, \n",
    "                                       filter = ['url', 'author', 'title', 'body', 'created', \n",
    "                                                'num_comments', 'score', 'link_flair_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del author_post_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del author_post_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull all comments made by dd post authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "On author 500 out of 6258\n",
      "============================\n",
      "On author 1000 out of 6258\n",
      "============================\n",
      "On author 1500 out of 6258\n",
      "============================\n",
      "On author 2000 out of 6258\n",
      "============================\n",
      "On author 2500 out of 6258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimmiegeorge/opt/anaconda3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 522\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "On author 3000 out of 6258\n",
      "============================\n",
      "On author 3500 out of 6258\n",
      "============================\n",
      "On author 4000 out of 6258\n",
      "============================\n",
      "On author 4500 out of 6258\n",
      "============================\n",
      "On author 5000 out of 6258\n",
      "============================\n",
      "On author 5500 out of 6258\n",
      "============================\n",
      "On author 6000 out of 6258\n"
     ]
    }
   ],
   "source": [
    "# loop through authors\n",
    "for i in range(421, len(authors)):\n",
    "    \n",
    "   \n",
    "    a = authors[i]\n",
    "    \n",
    "    # print counter \n",
    "    if i % 500 == 0:\n",
    "        print('============================')\n",
    "        print('On author ' + str(i) + ' out of ' + str(len(authors)))\n",
    "    \n",
    "    # search for all submissions \n",
    "    comm_search = api.search_comments(subreddit = 'wallstreetbets', author = a, \n",
    "                                       filter = ['id','body', 'author','parent_id', 'score', 'created'])\n",
    "    # loop through sub_search \n",
    "    for comment in comm_search:\n",
    "        \n",
    "        # if comment body not equal to removed \n",
    "        if comment.body != '[removed]':\n",
    "        \n",
    "            # pull all needed data\n",
    "            _id = comment.id\n",
    "            author = comment.author\n",
    "            body = comment.body\n",
    "            parent_id = comment.parent_id\n",
    "            score = comment.score\n",
    "            created = comment.created\n",
    "\n",
    "            # append to author post list \n",
    "            author_comment_list.append((a, _id, author, body, parent_id, score, created))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to df \n",
    "author_comment_df = pd.DataFrame(author_comment_list, columns = ['author', 'comment_id', 'author2', 'body', \n",
    "                                                          'parent_id', 'score', 'created'])\n",
    "\n",
    "\n",
    "# remove duplicate posts (from re-starting loop)\n",
    "author_comment_df = author_comment_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_comment_df.to_pickle('/Volumes/Elements/Research/Reddit_Credibility/Data/comments_by_dd_authors.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
